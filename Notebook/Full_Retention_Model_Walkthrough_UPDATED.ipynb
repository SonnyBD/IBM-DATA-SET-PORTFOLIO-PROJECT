{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8308f4",
   "metadata": {},
   "source": [
    "# üìä Predicting Employee Retention Risk\n",
    "\n",
    "This notebook walks through a complete machine learning pipeline to predict employee attrition risk using the IBM HR Analytics dataset.\n",
    "\n",
    "The goal is to support HR decision-making by identifying at-risk employees and explaining the factors contributing to their attrition risk using interpretable machine learning techniques."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e649bec333c41188"
  },
  {
   "cell_type": "markdown",
   "id": "90295ffd",
   "metadata": {},
   "source": [
    "## 1. Load & Preview Data\n",
    "\n",
    "This dataset includes demographic, performance, and engagement-related features from IBM HR records. It provides the foundation for modeling employee attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('../data/IBM_Test_Project_Preprocessed_Data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34188d",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "We create composite features (e.g., an engagement index, promotion rate) that capture employee behavior and experience more effectively than raw columns alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "if \"JobSatisfaction\" in df.columns and \"WorkLifeBalance\" in df.columns:\n",
    "    df[\"Engagement_Index\"] = df[\"JobSatisfaction\"] * df[\"WorkLifeBalance\"]\n",
    "if \"YearsSinceLastPromotion\" in df.columns and \"YearsAtCompany\" in df.columns:\n",
    "    df[\"Promotion_Rate\"] = df[\"YearsSinceLastPromotion\"] / (df[\"YearsAtCompany\"] + 1)\n",
    "if \"Doing_Overtime\" in df.columns and \"Laboratory_Technician\" in df.columns:\n",
    "    df[\"Overtime_SensitiveRole\"] = df[\"Doing_Overtime\"] * df[\"Laboratory_Technician\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0dcf5",
   "metadata": {},
   "source": [
    "## 3. Prepare Data\n",
    "\n",
    "We standardize continuous variables, remove identifiers, and separate features (X) from the target variable (`Retained`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifier and separate target\n",
    "X = df.drop(columns=[\"Retained\", \"EmployeeNumber\"], errors='ignore')\n",
    "y = df[\"Retained\"]\n",
    "\n",
    "# Scale features\n",
    "scaler_std = StandardScaler()\n",
    "X_scaled = scaler_std.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ab309",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split and SMOTE\n",
    "\n",
    "Attrition is rare (~16%), so we use SMOTE (Synthetic Minority Oversampling) to balance the classes and prevent bias toward predicting ‚Äúretained‚Äù employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42336f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fb492",
   "metadata": {},
   "source": [
    "## 5. Feature Selection (RFE)\n",
    "\n",
    "We apply Recursive Feature Elimination (RFE) using a Random Forest base model to select the 20 most predictive features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af43cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_selector = RFE(estimator=RandomForestClassifier(), n_features_to_select=20)\n",
    "rfe_selector.fit(X_scaled, y)\n",
    "X_selected = X.loc[:, rfe_selector.support_]\n",
    "X_scaled_selected = scaler_std.fit_transform(X_selected)\n",
    "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_scaled_selected, y, test_size=0.2, random_state=42)\n",
    "X_train_sel_res, y_train_sel_res = sm.fit_resample(X_train_sel, y_train_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a787c5",
   "metadata": {},
   "source": [
    "## 6. Model Training & Calibration\n",
    "\n",
    "We use GridSearchCV to tune a Random Forest model and apply Platt scaling via sigmoid calibration to improve the quality of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "rf_grid.fit(X_train_sel_res, y_train_sel_res)\n",
    "rf_model = rf_grid.best_estimator_\n",
    "\n",
    "calibrated_rf = CalibratedClassifierCV(rf_model, method='sigmoid', cv=5)\n",
    "calibrated_rf.fit(X_train_sel_res, y_train_sel_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea87d5",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "We assess the model using accuracy, precision, recall, F1-score, and confusion matrix. High recall is prioritized to reduce false negatives (i.e., missed attrition cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d51272",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = calibrated_rf.predict(X_test_sel)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_sel, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_sel, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test_sel, y_pred_rf))\n",
    "\n",
    "cv_scores = cross_val_score(calibrated_rf, X_scaled_selected, y, cv=5, scoring='f1')\n",
    "print(\"Cross-validated F1 scores:\", cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e9bf2",
   "metadata": {},
   "source": [
    "## 8. Generate Retention Risk Scores\n",
    "\n",
    "We convert predicted probabilities into retention risk scores using a Min-Max scaler and then assign each employee a risk level (Low, Moderate, High)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Predicted_Prob_Stay\"] = calibrated_rf.predict_proba(X_scaled_selected)[:, 1]\n",
    "scaler = MinMaxScaler()\n",
    "df[\"Retention_Risk\"] = 1 - scaler.fit_transform(df[[\"Predicted_Prob_Stay\"]])\n",
    "\n",
    "percentile_90 = df[\"Retention_Risk\"].quantile(0.90)\n",
    "percentile_50 = df[\"Retention_Risk\"].quantile(0.50)\n",
    "\n",
    "df[\"Risk_Level\"] = pd.cut(df[\"Retention_Risk\"], bins=[-float(\"inf\"), percentile_50, percentile_90, float(\"inf\")], labels=[\"Low Risk\", \"Moderate Risk\", \"High Risk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ada5d1",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "The final dataset is exported as an Excel file and includes employee retention probabilities, risk scores, and assigned risk tiers for HR stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=\"Retention_Risk\", ascending=False)\n",
    "df_sorted.to_excel(\"../outputs/Retention_Risk_Analysis_Output.xlsx\", index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b63737ae9ada1c10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. SHAP Explainability\n",
    "\n",
    "We use SHAP values to interpret the model's predictions. This allows us to identify the top drivers of attrition and communicate model results transparently to HR teams."
   ],
   "id": "66e46fb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "explainer = shap.Explainer(rf_model, X_scaled_selected)\n",
    "shap_values = explainer(X_scaled_selected, check_additivity=False)\n",
    "shap.summary_plot(shap_values, X_selected, plot_type=\"bar\")"
   ],
   "id": "ab2d3566"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Conclusion & Business Takeaways\n",
    "\n",
    "- The calibrated Random Forest model achieved **88% accuracy** and **98% recall** in identifying high-risk employees.\n",
    "- **Top predictors** of attrition included Overtime, Promotion Rate, and Job Satisfaction.\n",
    "- Employees were categorized into **Low**, **Moderate**, and **High Risk** tiers based on predicted probabilities.\n",
    "- **SHAP values** made the model transparent, enabling HR to understand and trust the predictions.\n",
    "\n",
    "This project demonstrates how People Analytics and interpretable machine learning can be leveraged to reduce turnover and drive strategic retention planning."
   ],
   "id": "dc451e5e"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
